import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
import chardet
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# è¨­å®š matplotlib ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False

# ========= ä½¿ç”¨è€…è¨­å®šå€ =========
file_path = r'D:\_Document\Others\_FlawDetection\ScrewData\Training\RawData\603_611_618_630_709.csv'  # â† ä¿®æ”¹ç‚ºä½ çš„ CSV æª”æ¡ˆ
target_column = 'ç ´å£æ‰­åŠ›'
use_all_data = False  # True: å…¨è³‡æ–™è¨“ç·´èˆ‡æ¸¬è©¦ï¼›False: 8:2 åˆ‡åˆ†
# ==============================

# è‡ªå‹•åµæ¸¬æª”æ¡ˆç·¨ç¢¼
#with open(file_path, 'rb') as f:
#    encoding = chardet.detect(f.read())['encoding']

df = pd.read_csv(file_path, encoding='big5')

# åŸå§‹ç‰¹å¾µ
feature_columns = ['é ­å¾‘', 'é ­åš', 'ç‰™å¾‘', 'ç‰™é•·', 'é‡æ·±', 'æ§½å¯¬', 'NYLOK', 'å°æ§½å¯¬', 'ç¡¬åº¦']
# feature_columns = ['é ­å¾‘', 'é ­åš', 'ç‰™å¾‘', 'ç‰™é•·', 'é‡æ·±', 'æ§½å¯¬', 'NYLOK', 'å°æ§½å¯¬', 'ç¡¬åº¦', 'T_shear']

# åŠ å…¥äº¤å‰ç‰¹å¾µ
#df['é ­å¾‘xç‰™å¾‘'] = df['é ­å¾‘'] * df['ç‰™å¾‘']
#df['ç¡¬åº¦å¹³æ–¹'] = df['ç¡¬åº¦'] ** 2
#df['NYLOKxç‰™é•·'] = df['NYLOK'] * df['ç‰™é•·']
#feature_columns += ['é ­å¾‘xç‰™å¾‘', 'ç¡¬åº¦å¹³æ–¹', 'NYLOKxç‰™é•·']

# åŸå§‹ç‰¹å¾µï¼ˆåƒ…é¸æ“‡éƒ¨åˆ†ç‰¹å¾µï¼‰
# selected_feature_columns = ['ç¡¬åº¦']  # â† ä¿®æ”¹ç‚ºä½ éœ€è¦çš„ç‰¹å¾µ

X = df[feature_columns]
# X = df[selected_feature_columns]
y = df[target_column]

# è³‡æ–™åˆ‡åˆ†
if use_all_data:
    X_train, X_test, y_train, y_test = X, X, y, y
    print("âš ï¸ ä½¿ç”¨å…¨éƒ¨è³‡æ–™åšè¨“ç·´èˆ‡æ¸¬è©¦ï¼ˆè©•ä¼°çµæœå¯èƒ½åé«˜ï¼‰")
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print("âœ… ä½¿ç”¨ 80/20 è¨“ç·´/æ¸¬è©¦åˆ†å‰²")

# è¨“ç·´æ¨¡å‹
#model = XGBRegressor(random_state=42)
# === å»ºç«‹å¼·åŒ– XGBoost æ¨¡å‹ ===
model = XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
model.fit(X_train, y_train)

# é æ¸¬èˆ‡è©•ä¼°
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nğŸ“Š XGBoost è©•ä¼°çµæœï¼š")
print(f"MAE  : {mae:.4f}")
print(f"RMSE : {rmse:.4f}")
print(f"RÂ²   : {r2:.4f}")

# é æ¸¬ vs å¯¦éš›åœ–
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, c='blue', alpha=0.6, label='é æ¸¬é»')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='ç†æƒ³ç·š')
plt.xlabel('å¯¦éš›å€¼')
plt.ylabel('é æ¸¬å€¼')
plt.title('é æ¸¬ vs å¯¦éš›')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# æ®˜å·®åœ–
residuals = y_test - y_pred
plt.figure(figsize=(6, 4))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='r', linestyle='--')
plt.xlabel('é æ¸¬å€¼')
plt.ylabel('æ®˜å·® (å¯¦éš› - é æ¸¬)')
plt.title('æ®˜å·®åœ–')
plt.grid(True)
plt.tight_layout()
plt.show()

# SHAP åˆ†æ
print("ğŸ“ˆ SHAP åˆ†æç‰¹å¾µé‡è¦æ€§...")
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test, plot_type='bar')
